<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero is-light">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">TextCenGen: Attention-Guided Text-Centric Background Adaptation for Text-to-Image Generation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Tianyi_Liang2" target="_blank">Tianyi Liang</a><sup>†</sup>,</span>
              <span class="author-block">
                Jiangqi Liu<sup>†</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com.co/citations?user=L444eYsAAAAJ" target="_blank">Yifei Huang</a>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Gyw0IL0AAAAJ" target="_blank">Shiqi Jiang</a>,</span>
              <span class="author-block">
                Jianshen Shi,</span>
              <span class="author-block">
                Changbo Wang,</span>
              <span class="author-block">
                <a href="mailto:chli@cs.ecnu.edu.cn" target="_blank">Chenhui Li</a><sup>*</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">College of Computer Science and Technology, East China Normal University, Shanghai, China</span><br>
              <span class="author-block">Shanghai Institute of AI Education, Shanghai, China</span><br>
              <span class="author-block">Shanghai Artificial Intelligence Laboratory, Shanghai, China</span><br>
              <span class="eql-cntrb"><small><br><sup>†</sup>Equal Contribution, <sup>*</sup>Corresponding Author</small></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">ICML 2025</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                   <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2404.11824" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Supplementary PDF link -->
              <!-- <span class="link-block">
                <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Supplementary</span>
              </a>
            </span> -->

            <!-- Github link -->
            <span class="link-block">
              <a href="https://github.com/tianyilt/TextCenGen_Background_Adapt" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>

          <!-- ArXiv abstract Link -->
          <span class="link-block">
            <a href="https://arxiv.org/abs/2404.11824" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="ai ai-arxiv"></i>
            </span>
            <span>arXiv</span>
          </a>
        </span>
      </div>
    </div>
  </div>
</div>
</section>


<!-- 图片展示 -->
<section class="hero is-small is-white">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3"></h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-image">
            <!-- 替换为你的图片文件 -->
            <img src="static/images/1.png" alt="描述文字" style="width: 100%; height: auto;">
          </div>
          <p class="image-description">
            TextCenGen is a training-free method designed to generate text-friendly images. By using a simple text prompt and a planned
blank region as inputs, TextCenGen creates images that satisfy the prompt and provide sufficient blank space in the target region. For
example, the text-friendly T2I approach helps users customize their favored text-friendly wallpapers for mobile devices with any T2I
model, avoiding visual confusion caused by the main objects overlapping with UI components.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End 图片展示 -->

<!-- Paper abstract -->
<section class="section hero is-white">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-image (T2I) generation has made remarkable progress in producing high-quality images,
but a fundamental challenge remains: creating
backgrounds that naturally accommodate text
placement without compromising image quality.
This capability is non-trivial for real-world applications like graphic design, where clear visual
hierarchy between content and text is essential.
Prior work has primarily focused on arranging layouts within existing static images, leaving unexplored the potential of T2I models for generating
text-friendly backgrounds. We present TextCenGen, a training-free dynamic background adaptation in the blank region for text-friendly image
generation. Instead of directly reducing attention
in text areas, which degrades image quality, we
relocate conflicting objects before background optimization. Our method analyzes cross-attention
maps to identify conflicting objects overlapping
with text regions and uses a force-directed graph
approach to guide their relocation, followed by
attention excluding constraints to ensure smooth
backgrounds. Our method is plug-and-play, requiring no additional training while well balancing both semantic fidelity and visual quality. Evaluated on our proposed text-friendly T2I benchmark of 27,000 images across three seed datasets,
TextCenGen outperforms existing methods by
achieving 23% lower saliency overlap in text
regions while maintaining 98% of the original
semantic fidelity measured by CLIP score and
our proposed Visual-Textual Concordance Metric
(VTCM).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Demos -->
<section class="hero is-small is-white">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Demos</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-4">Logo with Adaptive Natural Background</h3>
          <div class="publication-image">
            <img src="static/images/tcgteaser.png" alt="Logo with Adaptive Natural Background"
              style="width: 100%; height: auto;">
          </div>

          <h3 class="title is-4">Mobile Devices Wallpaper</h3>
          <div class="publication-image">
            <img src="static/images/teaser_mobile.png" alt="Mobile Devices Wallpaper"
              style="width: 100%; height: auto;">
          </div>

          <!-- <h3 class="title is-4">Comparison with Previous Works</h3>
          <div class="publication-image">
            <img src="static/images/result_compare_2p6m.jpg" alt="Comparison with Previous Works"
              style="width: 100%; height: auto;">
          </div> -->

          <h3 class="title is-4">More Results</h3>
          <div class="publication-image">
            <img src="static/images/more_results.jpg" alt="More Results" style="width: 100%; height: auto;">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Demos -->





<!-- Youtube video -->
<!-- 图片展示 -->
<section class="hero is-small is-white">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Force-Directed Cross-Attention Guidance</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-image">
            <!-- 替换为你的图片文件 -->
            <img src="static/images/3.png" alt="描述文字" style="width: 100%; height: auto;">
          </div>
          <p class="image-description">
            In our approach, the model receives a blank region (R) denoted as red-dotted area, and a text prompt as its inputs. The prompt
is then used concurrently in a Text-to-Image (T2I) model to generate both an original image and a result image. During each step of
the diffusion model's denoising process, the cross-attention map from the U-Net associated with the original image is used to direct the
denoising of the result image in the form of a loss function. Throughout this procedure, a conflict detector identifies objects that could
potentially conflict with R. To mitigate such conflicts, a force-directed graph method is applied to spatially repel these objects, ensuring
that the area reserved for the text prompt remains unoccupied. To further enhance the smoothness of the attention mechanism, a spatial
excluding cross-attention constraint is integrated into the cross-attention map.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>




  <!-- 图片展示 -->
<section class="hero is-small is-white">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3"></h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-image">
            <!-- 替换为你的图片文件 -->
            <img src="static/images/4.png" alt="描述文字" style="width: 100%; height: auto;">
          </div>
          <p class="image-description">
            Illustration of four set relationships and their associated
forces. The Repulsive Force separates object and text centroids
during intersections (a1) and object in text (a2). The Margin Force
(b) and Warping Force (c) prevent boundary overstepping. Text
within object regions (a4) requires cooperation between force and
attention constraint. Separation (a3) isn't required to process.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End 图片展示 -->

<!-- 图片展示 -->
<section class="hero is-small is-white">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Comparison with Previous Works</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-image">
            <!-- 替换为你的图片文件 -->
            <img src="static/images/result_compare_2p6m.jpg" alt="描述文字" style="width: 100%; height: auto;">
          </div>
          <p class="image-description">
            We compared TextCenGen with several potential models to
            evaluate its efficiency. The baseline models included: Native
            Stable Diffusion (Rombach et al., 2022), Dall-E 3 (Ramesh
            et al., 2022), AnyText (Tuo et al., 2023) and Desigen (Weng
            et al., 2024). Dall-E used the prompt "text-friendly in the
            {position}" to specify the region R. Similar to AnyText, we
            chose to randomly generate several masks in a fixed pattern
            across the map to simulate regions need to be edited.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End 图片展示 -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{liang2025textcengen,
  title     = {TextCenGen: Attention-Guided Text-Centric Background Adaptation for Text-to-Image Generation},
  author    = {Liang, Tianyi and Liu, Jiangqi and Huang, Yifei and Jiang, Shiqi and Shi, Jianshen and Wang, Changbo and Li, Chenhui},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
